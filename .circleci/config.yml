version: 2.1

### It's a good practice to keep your commands at the top of the config file. In this project, you'll need at least 2 commands:

commands:
  destroy-environment:
    description: Destroy all cloudformation stacks given a workflow ID.
    steps:
      - run:
          name: Delete all files in the current S3 bucket
          command: |
            if aws s3api head-bucket --bucket udapeople-zypher9-${CIRCLE_WORKFLOW_ID}
            then
              aws s3 rm s3://udapeople-zypher9-${CIRCLE_WORKFLOW_ID}/ --recursive
            fi
          when: on_fail
      - run:
          name: Delete the current AWS CloudFormation stacks
          command: |
            if aws cloudformation wait stack-exists --stack-name udapeople-cloudfront
            then 
              aws cloudformation delete-stack --stack-name udapeople-cloudfront
            fi
            if aws cloudformation wait stack-exists --stack-name udapeople-frontend-${CIRCLE_WORKFLOW_ID}
            then 
              aws cloudformation delete-stack --stack-name udapeople-frontend-${CIRCLE_WORKFLOW_ID}
            fi
            if aws cloudformation wait stack-exists --stack-name udapeople-backend-${CIRCLE_WORKFLOW_ID}
            then
              aws cloudformation delete-stack --stack-name udapeople-backend-${CIRCLE_WORKFLOW_ID}
            fi
          when: on_fail

  revert-migration:
    description: Revert the latest migration
    steps:
      - run:
          name: Get the public DNS of EC2 instance from https://memstash.io/
          command: |
            PUBLIC_DNS=$(curl -H "token: ${CIRCLE_WORKFLOW_ID}" --request GET https://api.memstash.io/values/public_dns)
            echo ${PUBLIC_DNS}
            cd .circleci/ansible/
            echo "[all]" > ./inventory
            echo ${PUBLIC_DNS} >> ./inventory
            cat ./inventory
          when: on_fail
      - add_ssh_keys:
          fingerprints: ["a0:2e:f2:a1:47:58:b7:00:6d:c5:07:a9:38:20:2c:e6"]
      - run:
          name: Revert the last migration
          command: |
            printenv >> ./backend/.env
            cd .circleci/ansible/
            ansible-playbook -i ./inventory db_rollback.yml
          when: on_fail

parameters:
  circleci_image:
    type: string
    default: circleci/node:latest
  py_alp_image:
    type: string
    default: python:3.7-alpine3.11

jobs:
  build-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            cd frontend
            npm install
      - run:
          name: Compile the code
          command: |
            cd frontend
            npm run build

  test-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            cd frontend
            npm install
      - run:
          name: Test the code
          command: |
            cd frontend
            npm run test

  scan-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            cd frontend
            npm install
      - run:
          name: Analyze the code
          command: |
            cd frontend
            npm audit fix --audit-level=critical --force

  build-backend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            cd backend
            npm install
      - run:
          name: Compile the code
          command: |
            cd backend
            npm run build

  test-backend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            cd backend
            npm install
      - run:
          name: Test the code
          command: |
            cd backend
            npm run test

  scan-backend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            cd backend
            npm install
      - run:
          name: Analyze the code
          command: |
            cd backend
            npm audit fix --audit-level=critical --force

  create-new-infrastructure:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Create frontend infrastructure via AWS CloudFormation
          command: |
            aws cloudformation deploy \
              --stack-name udapeople-frontend-${CIRCLE_WORKFLOW_ID} \
              --template-file .circleci/files/frontend.yml \
              --region ${AWS_DEFAULT_REGION} \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID}"
      - run:
          name: Create backend infrastructure via AWS CloudFormation
          command: |
            aws cloudformation deploy \
              --stack-name udapeople-backend-${CIRCLE_WORKFLOW_ID} \
              --template-file .circleci/files/backend.yml \
              --region ${AWS_DEFAULT_REGION} \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID}"
      - run:
          name: Get and save public DNS of EC2 instance to https://memstash.io/
          command: |
            PUBLIC_DNS=$(aws ec2 describe-instances --region ${AWS_DEFAULT_REGION} --filters 'Name=tag:Name,Values=udapeople-backend-ec2-*' --query "Reservations[*].Instances[0].PublicDnsName" --output text)
            echo ${PUBLIC_DNS}
            curl -H "Content-Type: text/plain" \
               -H "token: ${CIRCLE_WORKFLOW_ID}" \
               --request PUT \
               --data ${PUBLIC_DNS} \
               https://api.memstash.io/values/public_dns
      - destroy-environment

  deploy-backend:
    docker:
      - image: python:3.7-alpine3.11
    steps:
      - checkout
      - run:
          name: Install system dependencies
          command: |
            apk add --no-cache curl
            apk add --no-cache --upgrade bash
            apk add --no-cache --update ansible
            apk add --no-cache openssh-client
            pip3 install awscli
      - run:
          name: Get the public DNS of EC2 instance from https://memstash.io/
          command: |
            PUBLIC_DNS=$(curl -H "token: ${CIRCLE_WORKFLOW_ID}" --request GET https://api.memstash.io/values/public_dns)
            echo ${PUBLIC_DNS}
            cd .circleci/ansible/
            echo "[all]" > ./inventory
            echo ${PUBLIC_DNS} >> ./inventory
            cat ./inventory
      - add_ssh_keys:
          fingerprints: ["a0:2e:f2:a1:47:58:b7:00:6d:c5:07:a9:38:20:2c:e6"]
      - run:
          name: Configure server via ansible to deploy the backend
          command: |
            printenv >> ./backend/.env
            cd .circleci/ansible/
            ansible-playbook -i ./inventory deploy.yml
      - revert-migration
      - destroy-environment

  deploy-frontend:
    docker:
      - image: python:3.7-alpine3.11
    steps:
      - checkout
      - run:
          name: Install system dependencies
          command: |
            apk add --no-cache curl
            apk add --no-cache --upgrade bash
            apk add --no-cache --update npm
            pip3 install awscli
      - run:
          name: Build the frontend
          command: |
            PUBLIC_DNS=$(curl -H "token: ${CIRCLE_WORKFLOW_ID}" --request GET https://api.memstash.io/values/public_dns)
            echo ${PUBLIC_DNS}
            export API_URL="http://${PUBLIC_DNS}:3030"
            echo API_URL=${API_URL}
            cd frontend
            npm install
            npm run build
      - run:
          name: Copy built frontend files to the S3 bucket
          command: |
            aws s3 cp ./frontend/dist s3://udapeople-s3bucket-${CIRCLE_WORKFLOW_ID}/ --recursive
      - destroy-environment

  smoke-test:
    docker:
      - image: python:3.7-alpine3.11
    steps:
      - checkout
      - run:
          name: Install system dependencies
          command: |
            apk add --no-cache curl
            apk add --no-cache --upgrade bash
            apk add --no-cache --update ansible
            apk add --no-cache openssh-client
            pip3 install awscli
      - run:
          name: Smoke test on frontend
          command: |
            URL="http://udapeople-s3bucket-${CIRCLE_WORKFLOW_ID}.s3-website.us-east-2.amazonaws.com/#/employees"
            if curl -s ${URL} | grep "Welcome"
            then
              return 0
            else
              return 1
            fi
      - run:
          name: Smoke test on backend
          command: |
            PUBLIC_DNS=$(curl -H "token: ${CIRCLE_WORKFLOW_ID}" --request GET https://api.memstash.io/values/public_dns)
            echo ${PUBLIC_DNS}
            if curl -s "http://${PUBLIC_DNS}:3030/api/status" | grep "ok"
            then
              return 0
            else
              return 1
            fi
      - revert-migration
      - destroy-environment

  delete-infrastructure:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Promote the new deployment and delete the old one
          command: |
            OLD_CIRCLE_WORKFLOW_ID=$(aws cloudformation list-exports --query "Exports[?Name=='CircleCI-WorkflowID'].Value" --output text)
            echo OLD_CIRCLE_WORKFLOW_ID=${OLD_CIRCLE_WORKFLOW_ID}
            STACKS=$(aws cloudformation list-stacks --query "StackSummaries[*].StackName" --stack-status-filter UPDATE_COMPLETE CREATE_COMPLETE --output text)
            echo STACKS=${STACKS}
            echo "Create CloudFront"
            aws cloudformation deploy \
              --stack-name udapeople-cloudfront \
              --template-file .circleci/files/cloudfront.yml \
              --region ${AWS_DEFAULT_REGION} \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID}"
            echo "Delete old infrastructure"
            if [ -n "${OLD_CIRCLE_WORKFLOW_ID}" ] && [[ "${STACKS[@]}" =~ "${OLD_CIRCLE_WORKFLOW_ID}" ]]
            then
              echo deleting all files at S3 bucket udapeople-zypher9-${OLD_CIRCLE_WORKFLOW_ID}
              aws s3 rm s3://udapeople-zypher9-${OLD_CIRCLE_WORKFLOW_ID}/ --recursive
              echo deleting stack udapeople-frontend-${OLD_CIRCLE_WORKFLOW_ID}
              aws cloudformation delete-stack --stack-name udapeople-frontend-${OLD_CIRCLE_WORKFLOW_ID}
              echo deleting stack udapeople-backend-${OLD_CIRCLE_WORKFLOW_ID}
              aws cloudformation delete-stack --stack-name udapeople-backend-${OLD_CIRCLE_WORKFLOW_ID}
            fi
      - destroy-environment

workflows:
  frontend:
    jobs:
      - build-frontend:
          filters:
            branches:
              ignore: master
      - test-frontend:
          requires:
            - build-frontend
          filters:
            branches:
              ignore: master
      - scan-frontend:
          requires:
            - test-frontend
          filters:
            branches:
              ignore: master
  backend:
    jobs:
      - build-backend:
          filters:
            branches:
              ignore: master
      - test-backend:
          requires:
            - build-backend
          filters:
            branches:
              ignore: master
      - scan-backend:
          requires:
            - test-backend
          filters:
            branches:
              ignore: master
  deploy:
    jobs:
      - create-new-infrastructure:
          filters:
            branches:
              only: master
      - deploy-backend:
          requires:
            - create-new-infrastructure
          filters:
            branches:
              only: master
      - deploy-frontend:
          requires:
            - deploy-backend
          filters:
            branches:
              only: master
      - smoke-test:
          requires:
            - deploy-frontend
          filters:
            branches:
              only: master
      - delete-infrastructure:
          requires:
            - smoke-test
          filters:
            branches:
              only: master
